{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76719c70-96cb-414b-a2ae-e5b867f66c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd  # kept for compatibility with your environment\n",
    "from numba import njit\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities: stable numerics\n",
    "# -----------------------------\n",
    "\n",
    "@njit(cache=True)\n",
    "def _stable_sigmoid_scalar(x):\n",
    "    # Numerically stable sigmoid for scalar x (Numba-friendly)\n",
    "    if x >= 0.0:\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    else:\n",
    "        ex = np.exp(x)\n",
    "        return ex / (1.0 + ex)\n",
    "\n",
    "@njit(cache=True)\n",
    "def _softplus_scalar(x):\n",
    "    # Numerically stable softplus: log(1 + exp(x))\n",
    "    if x > 0.0:\n",
    "        return x + np.log1p(np.exp(-x))\n",
    "    else:\n",
    "        return np.log1p(np.exp(x))\n",
    "\n",
    "def stable_sigmoid(x):\n",
    "    # Vectorized stable sigmoid for potential external use (unchanged from your code’s spirit)\n",
    "    x = np.asarray(x)\n",
    "    out = np.empty_like(x, dtype=np.float64)\n",
    "    pos = x >= 0\n",
    "    neg = ~pos\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-x[pos]))\n",
    "    exp_x = np.exp(x[neg])\n",
    "    out[neg] = exp_x / (1.0 + exp_x)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Core heavy kernels (Numba)\n",
    "# -----------------------------\n",
    "\n",
    "@njit(cache=True)\n",
    "def _iterate_combinations_grad_nll(beta, degrees_float, r):\n",
    "    \"\"\"\n",
    "    Stream all r-combinations of n indices without materializing the entire list.\n",
    "    Compute:\n",
    "      - grad: sum over combinations containing j of sigmoid(sum(beta_e)) for each j\n",
    "      - nll: sum softplus(sum(beta_e)) - beta·degrees\n",
    "    degrees_float must be float64.\n",
    "    \"\"\"\n",
    "    n = beta.shape[0]\n",
    "    grad = np.zeros(n, dtype=np.float64)\n",
    "    term1 = 0.0\n",
    "\n",
    "    # initial combination [0,1,2,...,r-1]\n",
    "    idx = np.arange(r, dtype=np.int64)\n",
    "\n",
    "    while True:\n",
    "        # sum beta over current combination\n",
    "        s = 0.0\n",
    "        for j in range(r):\n",
    "            s += beta[idx[j]]\n",
    "\n",
    "        sig = _stable_sigmoid_scalar(s)\n",
    "        sp = _softplus_scalar(s)\n",
    "\n",
    "        # accumulate gradient contributions\n",
    "        for j in range(r):\n",
    "            grad[idx[j]] += sig\n",
    "\n",
    "        # accumulate NLL term1\n",
    "        term1 += sp\n",
    "\n",
    "        # generate next combination\n",
    "        i = r - 1\n",
    "        # find rightmost index that can be incremented\n",
    "        while i >= 0 and idx[i] == i + n - r:\n",
    "            i -= 1\n",
    "        if i < 0:\n",
    "            break\n",
    "        idx[i] += 1\n",
    "        for j in range(i + 1, r):\n",
    "            idx[j] = idx[j - 1] + 1\n",
    "\n",
    "    # subtract degrees from gradient and compute term2 = beta·degrees\n",
    "    term2 = 0.0\n",
    "    for i in range(n):\n",
    "        grad[i] -= degrees_float[i]\n",
    "        term2 += beta[i] * degrees_float[i]\n",
    "\n",
    "    nll = term1 - term2\n",
    "    return grad, nll\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def _generate_degrees_from_beta(n, r, beta):\n",
    "    \"\"\"\n",
    "    Generate degrees for an r-uniform hypergraph by iterating all r-combinations.\n",
    "    For each combination e, include it with probability sigmoid(sum(beta_e)).\n",
    "    Returns degrees (int64). We do not store edges to save memory.\n",
    "    \"\"\"\n",
    "    degrees = np.zeros(n, dtype=np.int64)\n",
    "\n",
    "    idx = np.arange(r, dtype=np.int64)\n",
    "    while True:\n",
    "        s = 0.0\n",
    "        for j in range(r):\n",
    "            s += beta[idx[j]]\n",
    "        p = _stable_sigmoid_scalar(s)\n",
    "        if np.random.random() < p:\n",
    "            for j in range(r):\n",
    "                degrees[idx[j]] += 1\n",
    "\n",
    "        # next combination\n",
    "        i = r - 1\n",
    "        while i >= 0 and idx[i] == i + n - r:\n",
    "            i -= 1\n",
    "        if i < 0:\n",
    "            break\n",
    "        idx[i] += 1\n",
    "        for j in range(i + 1, r):\n",
    "            idx[j] = idx[j - 1] + 1\n",
    "\n",
    "    return degrees\n",
    "\n",
    "# -----------------------------\n",
    "# Public API (compat signatures)\n",
    "# -----------------------------\n",
    "\n",
    "def generate_beta(n, rho):\n",
    "    rng = np.random.default_rng(seed=None)\n",
    "    # mixture: with prob rho -> 0, else N(0,1)\n",
    "    mask = rng.random(n) < rho\n",
    "    betas = np.empty(n, dtype=np.float64)\n",
    "    betas[mask] = 0.0\n",
    "    betas[~mask] = rng.normal(size=(~mask).sum())\n",
    "    return betas\n",
    "\n",
    "def generate_r_uniform_hypergraph(n, r, beta):\n",
    "    \"\"\"\n",
    "    Kept for compatibility with your call-site.\n",
    "    We no longer store edges (to avoid huge memory). Return an empty list for edges.\n",
    "    \"\"\"\n",
    "    degrees = _generate_degrees_from_beta(n, r, beta)\n",
    "    edges = []  # unused in downstream; preserved to keep tuple unpacking\n",
    "    return edges, degrees\n",
    "\n",
    "def add_gaussian_noise(degrees, epsilon, delta, r):\n",
    "    c_squared = 2.0 * np.log(1.25 / delta)\n",
    "    sigma = np.sqrt(c_squared) * np.sqrt(r) / epsilon\n",
    "    noise = np.random.normal(0.0, sigma, size=degrees.shape)\n",
    "    return degrees + noise\n",
    "\n",
    "def neg_log_likelihood(beta, degrees, r):\n",
    "    \"\"\"\n",
    "    Provided for completeness; uses the same streamed exact computation.\n",
    "    \"\"\"\n",
    "    deg_float = degrees.astype(np.float64, copy=False)\n",
    "    _, nll = _iterate_combinations_grad_nll(beta, deg_float, r)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51fd6770-7009-4e40-a36a-7265f6ff4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(n, r, noisy_degrees, lr=0.01, max_iter=1000,\n",
    "                     tol=1e-4, verbose=True):\n",
    "    \n",
    "    beta = np.zeros(n, dtype=np.float64)\n",
    "    degrees_f = noisy_degrees.astype(np.float64, copy=False)\n",
    "    success = 0\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        grad, nll = _iterate_combinations_grad_nll(beta, degrees_f, r)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "        if grad_norm < tol:\n",
    "            print(f\"Iter {t}: grad norm = {grad_norm:.4f}, NLL = {nll:.4f}\")\n",
    "            success = 1\n",
    "            break\n",
    "\n",
    "        beta -= lr * grad\n",
    "\n",
    "        if verbose:# and (t % 50 == 0):\n",
    "            print(f\"Iter {t}: grad norm = {grad_norm:.4f}, NLL = {nll:.4f}\")\n",
    "\n",
    "    return beta, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dceb5726-95c7-430b-a3a3-10c5d24b416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(n, r, rho, delta, epsilon):\n",
    "    beta = generate_beta(n, rho)\n",
    "    _, degrees = generate_r_uniform_hypergraph(n, r, beta)\n",
    "\n",
    "    lr = 0.00001\n",
    "    max_iter = 200\n",
    "    l2_error = []\n",
    "    linf_error = []\n",
    "\n",
    "    if epsilon is None:\n",
    "        beta_hat_0, success = gradient_descent(n, r, degrees, lr, max_iter)\n",
    "        l2 = np.linalg.norm(beta - beta_hat_0)\n",
    "        l2_error.append(l2)\n",
    "        linf = np.abs(beta - beta_hat_0).max()\n",
    "        linf_error.append(linf)\n",
    "        print(l2, linf)\n",
    "        return l2_error, linf_error, success\n",
    "\n",
    "    noisy_degrees = add_gaussian_noise(degrees, epsilon, delta, r)\n",
    "    beta_hat, success = gradient_descent(n, r, noisy_degrees, lr, max_iter)\n",
    "    l2 = np.linalg.norm(beta - beta_hat)\n",
    "    l2_error.append(l2)\n",
    "    linf = np.abs(beta - beta_hat).max()\n",
    "    linf_error.append(linf)\n",
    "    print(l2, linf)\n",
    "    return l2_error, linf_error, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9065fa3-cdf1-404f-9580-a4a78ebb4e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running for r = 3, n = 750, epsilon = 2 -> \n",
      "\n",
      "Iter 0: grad norm = 954484.3677, NLL = 48542136.7735\n",
      "Iter 1: grad norm = 370467.6518, NLL = 42567556.3978\n",
      "Iter 2: grad norm = 217483.1743, NLL = 41654428.0626\n",
      "Iter 3: grad norm = 148485.2352, NLL = 41377029.0781\n",
      "Iter 4: grad norm = 107634.0083, NLL = 41266129.8033\n",
      "Iter 5: grad norm = 79939.3131, NLL = 41214427.8236\n",
      "Iter 6: grad norm = 60055.1545, NLL = 41187983.9902\n",
      "Iter 7: grad norm = 45449.1373, NLL = 41173534.7160\n",
      "Iter 8: grad norm = 34616.3776, NLL = 41165242.1576\n",
      "Iter 9: grad norm = 26557.1386, NLL = 41160276.5204\n",
      "Iter 10: grad norm = 20548.3275, NLL = 41157191.5187\n",
      "Iter 11: grad norm = 16063.7792, NLL = 41155206.4401\n",
      "Iter 12: grad norm = 12709.4658, NLL = 41153887.8906\n",
      "Iter 13: grad norm = 10193.2721, NLL = 41152985.1179\n",
      "Iter 14: grad norm = 8296.0598, NLL = 41152350.1058\n",
      "Iter 15: grad norm = 6855.1869, NLL = 41151892.1762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (c \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m     l2, linf, success \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     t \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[86], line 20\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(n, r, rho, delta, epsilon)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l2_error, linf_error, success\n\u001b[0;32m     19\u001b[0m noisy_degrees \u001b[38;5;241m=\u001b[39m add_gaussian_noise(degrees, epsilon, delta, r)\n\u001b[1;32m---> 20\u001b[0m beta_hat, success \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoisy_degrees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m l2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(beta \u001b[38;5;241m-\u001b[39m beta_hat)\n\u001b[0;32m     22\u001b[0m l2_error\u001b[38;5;241m.\u001b[39mappend(l2)\n",
      "Cell \u001b[1;32mIn[85], line 9\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(n, r, noisy_degrees, lr, max_iter, tol, verbose)\u001b[0m\n\u001b[0;32m      6\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m----> 9\u001b[0m     grad, nll \u001b[38;5;241m=\u001b[39m \u001b[43m_iterate_combinations_grad_nll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegrees_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(grad)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_norm \u001b[38;5;241m<\u001b[39m tol:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delta = 1e-5\n",
    "rho = 0.6\n",
    "r = 3\n",
    "e_list = [None, 2, 1.5, 1, 0.5, 0.1]\n",
    "result = []\n",
    "\n",
    "for n in [750]:\n",
    "    for e in e_list:\n",
    "        print(f\"\\n Running for r = {r}, n = {n}, epsilon = {e} -> \\n\")\n",
    "        with open(f\"l_2 error, n - {n}, r - {r}, e - {e}.csv\", 'w') as f1, \\\n",
    "             open(f\"l_inf error, n - {n}, r - {r}, e - {e}.csv\", 'w') as f2:\n",
    "            c = 0\n",
    "            t = 100\n",
    "            while (c < 20 and t > 0):\n",
    "                l2, linf, success = run_simulation(n, r, rho, delta, e)\n",
    "                t -= 1\n",
    "                if success == 1:\n",
    "                    c += 1\n",
    "                    # Write current row to both files (same filenames/format)\n",
    "                    f1.write(','.join(map(str, l2)) + '\\n')\n",
    "                    f2.write(','.join(map(str, linf)) + '\\n')\n",
    "                # keep your tracking print\n",
    "                print(\"t = \", 100 - t, \"vs c = \", c)\n",
    "        g = [r, n, e, 100 - t, c]\n",
    "        print(g)\n",
    "        result.append(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
