{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f2542ee-e59c-44e7-a2ad-1aaf13c09ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5f04a8f-1584-48a7-911e-59ea1168cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beta(n, rho):\n",
    "    rng = np.random.default_rng(seed=None)\n",
    "    betas = np.where(rng.random(n) < rho, 0.0, rng.normal(size=n))\n",
    "    return betas\n",
    "\n",
    "def generate_r_uniform_hypergraph(n, r, beta):\n",
    "    edges = []\n",
    "    degrees = np.zeros(n, dtype=int)\n",
    "\n",
    "    for comb in itertools.combinations(range(n), r):\n",
    "        weight_sum = beta[list(comb)].sum()\n",
    "        p = np.exp(weight_sum) / (1 + np.exp(weight_sum))\n",
    "        if np.random.rand() < p:\n",
    "            edges.append(comb)\n",
    "            for v in comb:\n",
    "                degrees[v] += 1\n",
    "    return edges, degrees\n",
    "\n",
    "\n",
    "def add_gaussian_noise(degrees, epsilon, delta, r):\n",
    "    c_squared = 2 * np.log(1.25 / delta)\n",
    "    sigma = np.sqrt(c_squared) * np.sqrt(r) / epsilon\n",
    "    noise = np.random.normal(0, sigma, size=degrees.shape)\n",
    "    return degrees + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "987f8d90-3fb0-4719-9d84-2db0ea84d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_sigmoid(x):\n",
    "    out = np.empty_like(x)\n",
    "    pos = x >= 0\n",
    "    neg = ~pos\n",
    "    out[pos] = 1 / (1 + np.exp(-x[pos]))\n",
    "    exp_x = np.exp(x[neg])\n",
    "    out[neg] = exp_x / (1 + exp_x)\n",
    "    return out\n",
    "    \n",
    "def neg_log_likelihood(beta, degrees, r):  \n",
    "    n = len(beta)\n",
    "    edges = itertools.combinations(range(n), r)\n",
    "\n",
    "    term1 = 0.0\n",
    "    for e in edges:\n",
    "        s = np.sum(beta[list(e)])\n",
    "        term1 += np.logaddexp(0, s)\n",
    "\n",
    "    term2 = np.dot(beta, degrees)\n",
    "    return term1 - term2\n",
    "\n",
    "\n",
    "def gradient_descent(n, r, noisy_degrees, lr=0.01, max_iter=1000,\n",
    "                     tol=1e-6, verbose=True):\n",
    "    beta = np.zeros(n)\n",
    "    combs = np.array(list(itertools.combinations(range(n), r)))\n",
    "    success = 0\n",
    "    for t in range(max_iter):\n",
    "        beta_sums = np.sum(beta[combs], axis=1)\n",
    "        sigm = stable_sigmoid(beta_sums)\n",
    "        grad = np.zeros(n)\n",
    "        for j in range(r):\n",
    "            np.add.at(grad, combs[:, j], sigm)\n",
    "\n",
    "        grad -= noisy_degrees\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "        if grad_norm < tol:\n",
    "            nll = neg_log_likelihood(beta, noisy_degrees, r)\n",
    "            print(f\"Iter {t}: grad norm = {grad_norm:.4f}, NLL = {nll:.4f}\")\n",
    "            success = 1\n",
    "            break\n",
    "        beta -= lr * grad\n",
    "\n",
    "        if verbose and t % 100 == 0:\n",
    "            nll = neg_log_likelihood(beta, noisy_degrees, r)\n",
    "            print(f\"Iter {t}: grad norm = {grad_norm:.4f}, NLL = {nll:.4f}\")\n",
    "    return beta, success\n",
    "\n",
    "\n",
    "def run_simulation(n, r, rho, delta, epsilon):\n",
    "    beta = generate_beta(n, rho)\n",
    "    edges, degrees = generate_r_uniform_hypergraph(n, r, beta)\n",
    "\n",
    "    lr = 0.0001\n",
    "    max_iter = 3000\n",
    "    l2_error = []\n",
    "    linf_error = []\n",
    "\n",
    "    if epsilon == None:\n",
    "        beta_hat_0, success = gradient_descent(n, r, degrees, lr, max_iter)\n",
    "        l2 = np.linalg.norm(beta - beta_hat_0)\n",
    "        l2_error.append(l2)\n",
    "        linf = np.abs(beta - beta_hat_0).max()\n",
    "        linf_error.append(linf)\n",
    "        print(l2, linf)\n",
    "        return l2_error, linf_error, success\n",
    "    \n",
    "    noisy_degrees = add_gaussian_noise(degrees, epsilon, delta, r)\n",
    "    beta_hat, success = gradient_descent(n, r, noisy_degrees, lr, max_iter)\n",
    "    l2 = np.linalg.norm(beta - beta_hat)\n",
    "    l2_error.append(l2)\n",
    "    linf = np.abs(beta - beta_hat).max()\n",
    "    linf_error.append(linf)\n",
    "    print(l2, linf)\n",
    "    \n",
    "    return l2_error, linf_error, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d90d1f5-0da0-4d82-9e29-76b90d1ac041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running for r = 4, n = 100, epsilon = None -> \n",
      "\n",
      "Iter 0: grad norm = 189558.8656, NLL = 2425734.4101\n",
      "Iter 100: grad norm = 0.0884, NLL = 2158277.8875\n",
      "Iter 189: grad norm = 0.0000, NLL = 2158277.8875\n",
      "0.06348374869722358 0.019274832185846247\n",
      "t =  1 vs c =  0\n",
      "Iter 0: grad norm = 161983.1180, NLL = 2504748.5511\n",
      "Iter 100: grad norm = 0.0023, NLL = 2327328.8912\n",
      "Iter 148: grad norm = 0.0000, NLL = 2327328.8912\n",
      "0.05911184849141468 0.017010331668592116\n",
      "t =  2 vs c =  1\n",
      "Iter 0: grad norm = 180957.8244, NLL = 2451839.8661\n",
      "Iter 100: grad norm = 0.9166, NLL = 2218515.1151\n",
      "Iter 200: grad norm = 0.0001, NLL = 2218515.1151\n",
      "Iter 243: grad norm = 0.0000, NLL = 2218515.1151\n",
      "0.06359659738767097 0.016591696429638092\n",
      "t =  3 vs c =  2\n",
      "Iter 0: grad norm = 172743.0653, NLL = 2478597.1153\n",
      "Iter 100: grad norm = 0.0002, NLL = 2279996.5568\n",
      "Iter 127: grad norm = 0.0000, NLL = 2279996.5568\n",
      "0.04999569238388215 0.013952744927173773\n",
      "t =  4 vs c =  3\n",
      "Iter 0: grad norm = 160380.6075, NLL = 2516002.1430\n",
      "Iter 100: grad norm = 0.0046, NLL = 2350000.1420\n",
      "Iter 155: grad norm = 0.0000, NLL = 2350000.1420\n",
      "0.06002471384773991 0.016637531377897717\n",
      "t =  5 vs c =  4\n",
      "Iter 0: grad norm = 195619.9533, NLL = 2410796.3129\n",
      "Iter 100: grad norm = 0.5801, NLL = 2126888.4100\n",
      "Iter 200: grad norm = 0.0000, NLL = 2126888.4100\n",
      "Iter 227: grad norm = 0.0000, NLL = 2126888.4100\n",
      "0.05365385005609777 0.01502137616438049\n",
      "t =  6 vs c =  5\n",
      "Iter 0: grad norm = 170918.6396, NLL = 2501543.3463\n",
      "Iter 100: grad norm = 0.0012, NLL = 2324481.1973\n",
      "Iter 142: grad norm = 0.0000, NLL = 2324481.1973\n",
      "0.0596177655109951 0.01267604385460435\n",
      "t =  7 vs c =  6\n",
      "Iter 0: grad norm = 178066.5281, NLL = 2507460.7655\n",
      "Iter 100: grad norm = 0.0005, NLL = 2343204.2062\n",
      "Iter 136: grad norm = 0.0000, NLL = 2343204.2062\n",
      "0.05669417811620344 0.01648845395038301\n",
      "t =  8 vs c =  7\n",
      "Iter 0: grad norm = 188431.7233, NLL = 2470290.3027\n",
      "Iter 100: grad norm = 0.0828, NLL = 2258589.9774\n",
      "Iter 192: grad norm = 0.0000, NLL = 2258589.9774\n",
      "0.060773179710884744 0.018346410846302452\n",
      "t =  9 vs c =  8\n",
      "Iter 0: grad norm = 212003.0070, NLL = 2406312.6511\n",
      "Iter 100: grad norm = 0.0242, NLL = 2131078.1167\n",
      "Iter 171: grad norm = 0.0000, NLL = 2131078.1167\n",
      "0.06068977054079078 0.012968422238081201\n",
      "t =  10 vs c =  9\n",
      "Iter 0: grad norm = 188356.0469, NLL = 2443456.8342\n",
      "Iter 100: grad norm = 0.0246, NLL = 2204296.6599\n",
      "Iter 173: grad norm = 0.0000, NLL = 2204296.6599\n",
      "0.06511990903747752 0.014228364760442933\n",
      "t =  11 vs c =  10\n",
      "Iter 0: grad norm = 185374.8788, NLL = 2444461.4156\n",
      "Iter 100: grad norm = 0.7429, NLL = 2196080.4657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (c \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m         l2, linf, success \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         t \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt = \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m-\u001b[39m t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs c = \u001b[39m\u001b[38;5;124m\"\u001b[39m, c)\n",
      "Cell \u001b[1;32mIn[35], line 68\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(n, r, rho, delta, epsilon)\u001b[0m\n\u001b[0;32m     65\u001b[0m linf_error \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epsilon \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     beta_hat_0, success \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     l2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(beta \u001b[38;5;241m-\u001b[39m beta_hat_0)\n\u001b[0;32m     70\u001b[0m     l2_error\u001b[38;5;241m.\u001b[39mappend(l2)\n",
      "Cell \u001b[1;32mIn[35], line 34\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(n, r, noisy_degrees, lr, max_iter, tol, verbose, lam)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m     33\u001b[0m     beta_sums \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(beta[combs], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     sigm \u001b[38;5;241m=\u001b[39m \u001b[43mstable_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta_sums\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r):\n",
      "Cell \u001b[1;32mIn[35], line 5\u001b[0m, in \u001b[0;36mstable_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m pos \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mpos\n\u001b[1;32m----> 5\u001b[0m out[pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m exp_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x[neg])\n\u001b[0;32m      7\u001b[0m out[neg] \u001b[38;5;241m=\u001b[39m exp_x \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m exp_x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delta = 1e-5\n",
    "rho = 0.6\n",
    "r = 3\n",
    "e_list = [None, 2, 1.5, 1, 0.5]\n",
    "\n",
    "for n in [100, 250, 500, 750, 1000]:\n",
    "    for e in e_list:\n",
    "        print(f\"\\n Running for r = {r}, n = {n}, epsilon = {e} -> \\n\")\n",
    "        with open(f\"l_2 error, n - {n}, r - {r}, e - {e}.csv\", 'w') as f1, open(f\"l_inf error, n - {n}, r - {r}, e - {e}.csv\", 'w') as f2:\n",
    "            c = 0\n",
    "            t = 100\n",
    "            while (c < 20 and t > 0):\n",
    "                    l2, linf, success = run_simulation(n, r, rho, delta, e)\n",
    "                    t -= 1\n",
    "                    print(\"t = \", 100 - t, \"vs c = \", c)\n",
    "                    if success == 1:\n",
    "                        c += 1\n",
    "                        # Write current row to both files\n",
    "                        f1.write(','.join(map(str, l2)) + '\\n')\n",
    "                        f2.write(','.join(map(str, linf)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df883de6-6dc7-4713-9ded-36d9549ed419",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.loadtxt(f\"D:\\Projects\\Privacy and Beta Hypergraph\\simulation\\l_2 error, n - 1000, r - 2, e - 1.5.csv\", delimiter=',', skiprows = 1)\n",
    "m2 = np.loadtxt(f\"D:\\Projects\\Privacy and Beta Hypergraph\\simulation\\l_inf error, n - 1000, r - 2, e - 1.5.csv\", delimiter=',', skiprows = 1)\n",
    "print(m1.mean(), np.std(m1))\n",
    "print(m2.mean(), np.std(m2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
